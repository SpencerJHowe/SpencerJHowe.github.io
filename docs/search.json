[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SpencerJHowe.github.io",
    "section": "",
    "text": "Divisive Clustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nSpencer Howe\n\n\n\n\n\n\n  \n\n\n\n\nSilhouette Method\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nSpencer Howe\n\n\n\n\n\n\n  \n\n\n\n\nK-Means Clustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nSpencer Howe\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nSpencer Howe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Tracking a Senior Project with FamilySearch"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "Silhouette Method. Use Median Centroid Method\n\n\nHow to Utilize K-means Clustering for Anomaly Detection in Python\nFor my Senior Project this semester, I need to be able to leverage machine learning to aid in anomaly detection for a company. My assigned method was K-means learning and this article is going to explain how I’ve used it and how it can be used in general to help with anomaly detection across any data industry\n\n\nThe Power of K-Means in Anomaly Detection\nClustering itself is a process where we segment the data or group it to create meaningful groups that help us in machine learning or in analyzing the data or in detecting outliers. However, K-means clustering extends beyond normal data clustering—especially when it comes to identifying anomalies in datasets. But what makes K-means excel at this task?\nK-Means seeks to create clusters where data points are like one another. Anomalies are different from the majority of data points. When using K-Means for anomaly detection, you typically have one or two main cluster representing normal data and one or more smaller clusters representing anomalies. We can identify the smaller clusters as anomalous data because it doesn’t fit into our bigger main data clusters.\nHere’s a generic guide on how I used clustering for the project that I am working on:\n\nData Familiarization: Started with a csv file and identified the feature columns I needed to use as well as my target column I would use to see if the groupings were meaningful.\nData Standardization: K-means Clustering requires the data that we feed into it to be in numerical format and not to have any null values. I also applied a z-score function to all the features I was using from the scipy.stats function to standardize the data it was using.\nDetermine number of Clusters: Applying the elbow method, we determine the ideal number of clusters that should be used based off the features we are using. Based off the chart we produce we look at where the “elbow” is to state the number of clusters we want to use in our clustering of the data. In the example below we would obviously use 3 as the number of our clusters\n\ninertias = []\n\nfor i in range(1,10):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(allclusterdata)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(range(1,10), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\nCluster Assignment: I ran the algorithm that applies the K-means clustering and where we set the number of clusters we want. The code is also shown below:\n\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(allclusterdata)\n\nInterpretation of Results: Now I used the labels that the K-means clustering creates, labeled my data with it, and then checked to see what proportion of each of the clusters had “true” as their label in my target column and saw if there were any meaningful results or percentages where one group had a higher percentage of labeled anomaly data in it than the others. Thankfully, the clustering created a cluster that had ONLY my data labeled as being anomalous in it, which should be helpful in applying to other data that we have. The code below shows how to put our cluster labels onto our original data set:\n\nlabels = kmeans.labels_\ninterview_data[\"clusters\"] = labels.tolist()\n\n\nSome Real-World Applications\nAs stated in the beginning, anomaly detection using K-Means clustering has a wide range of real-world applications. Some of them are listed below:\n\nNetwork Security Detecting unusual patterns in network traffic to identify potential cyber threats and intrusions.\nFraud Detection Identifying fraudulent financial transactions or activities by spotting unusual patterns.\nIndustrial Equipment Monitoring Monitoring the health of industrial machinery by detecting anomalies in sensor data.\nQuality Control Checking for defects or anomalies in manufactured products on the production line.\nHealthcare Identifying unusual medical conditions or patient behaviors in healthcare datasets.\n\n\n\nConclusion\nK-Means clustering, while primarily known for its clustering capabilities, is a valuable tool for anomaly detection. Its ability to distinguish normal data from anomalies can be harnessed in various domains to improve security, efficiency, and decision-making processes. By mastering K-Means for anomaly detection, you can enhance your ability to detect and address exceptional cases within your data."
  },
  {
    "objectID": "posts/2nd Post - Silhouette Method/index.html",
    "href": "posts/2nd Post - Silhouette Method/index.html",
    "title": "Silhouette Method",
    "section": "",
    "text": "Understanding Clustering Quality with the Silhouette Method\nAs mentioned in my previous post, my senior project is to use clustering as a method to detect outiers in a dataframe. This time I will be talking about a different method used in clustering to help determine the correct amount of groups to make. This method is called the silhouette method.\n\n\nThe Silhouette Method: An Overview\nThe silhouette method evaluates the cohesion and separation of clusters, helping us understand how well-defined and distinct the clusters are. It assigns a silhouette score to each data point, reflecting its similarity to its own cluster compared to the nearest neighboring cluster.\n\n\nHow to Calculate the Silhouette Score\n\nCluster the data: First partition your data using k-means clustering that we used previously\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Assuming 'data' is your dataset\nk_values = range(2, 10)\nsilhouette_scores = []\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k)\n    labels = kmeans.fit_predict(data)\n    silhouette_avg = silhouette_score(data, labels)\n    silhouette_scores.append(silhouette_avg)\n\nCalculate Silhouette Score with the following fuction:\n\nfrom sklearn.metrics import silhouette_samples\nsilhouette_values = silhouette_samples(data, labels)\n\nEvaluate the Results:\n\naverage_silhouette = np.mean(silhouette_values)\n\n\nChoose the optimal value: The optimal value is the one that maximizes the silhouette score for your specific data base. If this value is at the edge of the tested value range set earlier, increase your range to make sure you don’t need a higher number of clusters\n\n\noptimal_k = k_values[np.argmax(silhouette_scores)]\n\n\nVisualize the scores: This code creates a plot that should show you where the scores peak\n\nimport matplotlib.pyplot as plt\n\nplt.plot(k_values, silhouette_scores, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Average Silhouette Score')\nplt.title('Silhouette Method for Optimal Clusters')\nplt.show()\n\nThe result may look something like this in which you would pick 2 as the amount of clusters you want to create\n\n\nConclusion\nThe silhouette method serves as a valuable tool for assessing the quality of clustering in a dataset. By leveraging silhouette scores and visualizations, we can make informed decisions about the number of clusters, ultimately enhancing the performance and interpretability of our clustering models. A large drawback to this method though is that it is very computationally expensive and takes a long time to run on larger datasets"
  },
  {
    "objectID": "posts/1st Post- K-means Clustering/index.html",
    "href": "posts/1st Post- K-means Clustering/index.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "Silhouette Method. Use Median Centroid Method\n\n\nHow to Utilize K-means Clustering for Anomaly Detection in Python\nFor my Senior Project this semester, I need to be able to leverage machine learning to aid in anomaly detection for a company. My assigned method was K-means learning and this article is going to explain how I’ve used it and how it can be used in general to help with anomaly detection across any data industry\n\n\nThe Power of K-Means in Anomaly Detection\nClustering itself is a process where we segment the data or group it to create meaningful groups that help us in machine learning or in analyzing the data or in detecting outliers. However, K-means clustering extends beyond normal data clustering—especially when it comes to identifying anomalies in datasets. But what makes K-means excel at this task?\nK-Means seeks to create clusters where data points are like one another. Anomalies are different from the majority of data points. When using K-Means for anomaly detection, you typically have one or two main cluster representing normal data and one or more smaller clusters representing anomalies. We can identify the smaller clusters as anomalous data because it doesn’t fit into our bigger main data clusters.\nHere’s a generic guide on how I used clustering for the project that I am working on:\n\nData Familiarization: Started with a csv file and identified the feature columns I needed to use as well as my target column I would use to see if the groupings were meaningful.\nData Standardization: K-means Clustering requires the data that we feed into it to be in numerical format and not to have any null values. I also applied a z-score function to all the features I was using from the scipy.stats function to standardize the data it was using.\nDetermine number of Clusters: Applying the elbow method, we determine the ideal number of clusters that should be used based off the features we are using. Based off the chart we produce we look at where the “elbow” is to state the number of clusters we want to use in our clustering of the data. In the example below we would obviously use 3 as the number of our clusters\n\ninertias = []\n\nfor i in range(1,10):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(allclusterdata)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(range(1,10), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\nCluster Assignment: I ran the algorithm that applies the K-means clustering and where we set the number of clusters we want. The code is also shown below:\n\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(allclusterdata)\n\nInterpretation of Results: Now I used the labels that the K-means clustering creates, labeled my data with it, and then checked to see what proportion of each of the clusters had “true” as their label in my target column and saw if there were any meaningful results or percentages where one group had a higher percentage of labeled anomaly data in it than the others. Thankfully, the clustering created a cluster that had ONLY my data labeled as being anomalous in it, which should be helpful in applying to other data that we have. The code below shows how to put our cluster labels onto our original data set:\n\nlabels = kmeans.labels_\ninterview_data[\"clusters\"] = labels.tolist()\n\n\nSome Real-World Applications\nAs stated in the beginning, anomaly detection using K-Means clustering has a wide range of real-world applications. Some of them are listed below:\n\nNetwork Security Detecting unusual patterns in network traffic to identify potential cyber threats and intrusions.\nFraud Detection Identifying fraudulent financial transactions or activities by spotting unusual patterns.\nIndustrial Equipment Monitoring Monitoring the health of industrial machinery by detecting anomalies in sensor data.\nQuality Control Checking for defects or anomalies in manufactured products on the production line.\nHealthcare Identifying unusual medical conditions or patient behaviors in healthcare datasets.\n\n\n\nConclusion\nK-Means clustering, while primarily known for its clustering capabilities, is a valuable tool for anomaly detection. Its ability to distinguish normal data from anomalies can be harnessed in various domains to improve security, efficiency, and decision-making processes. By mastering K-Means for anomaly detection, you can enhance your ability to detect and address exceptional cases within your data."
  },
  {
    "objectID": "posts/3rd Post - Agglomerative Clustering/index.html",
    "href": "posts/3rd Post - Agglomerative Clustering/index.html",
    "title": "Divisive Clustering",
    "section": "",
    "text": "Understanding Patterns in Data with Divisive Clustering\nIn this post, I’ll be looking at divisive clustering not only as a way to detect outliers in the data but also as a method of data analysis and exploration that could be applied to any dataset to help find patterns and increase understanding of a dataset\n\n\nDivisive Clustering: How it Works\nFirst, you start with a single cluster encompassing all data points. Then you split clusters based on dissimilarity until each data point forms a singleton cluster. That’s the whole process! We’ll see how this looks and understand it’s usefulness with some example data.\n\n\nCode Implementation\nfrom sklearn.datasets import make_blobs\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n\n# Perform divisive hierarchical clustering\nZ = linkage(X, method='ward')\ndendrogram(Z, orientation='top')\nplt.title('Divisive Hierarchical Clustering Dendrogram')\nplt.show()\n\n\nCluster Assignment: I ran the algorithm that applies the K-means clustering and where we set the number of clusters we want. The code is also shown below:\n\n\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(allclusterdata)\n\nInterpretation of Results: Now I used the labels that the K-means clustering creates, labeled my data with it, and then checked to see what proportion of each of the clusters had “true” as their label in my target column and saw if there were any meaningful results or percentages where one group had a higher percentage of labeled anomaly data in it than the others. Thankfully, the clustering created a cluster that had ONLY my data labeled as being anomalous in it, which should be helpful in applying to other data that we have. The code below shows how to put our cluster labels onto our original data set:\n\nlabels = kmeans.labels_\ninterview_data[\"clusters\"] = labels.tolist()\n\n\nSome Real-World Applications\nAs stated in the beginning, anomaly detection using K-Means clustering has a wide range of real-world applications. Some of them are listed below:\n\nNetwork Security Detecting unusual patterns in network traffic to identify potential cyber threats and intrusions.\nFraud Detection Identifying fraudulent financial transactions or activities by spotting unusual patterns.\nIndustrial Equipment Monitoring Monitoring the health of industrial machinery by detecting anomalies in sensor data.\nQuality Control Checking for defects or anomalies in manufactured products on the production line.\nHealthcare Identifying unusual medical conditions or patient behaviors in healthcare datasets.\n\n\n\nConclusion\nK-Means clustering, while primarily known for its clustering capabilities, is a valuable tool for anomaly detection. Its ability to distinguish normal data from anomalies can be harnessed in various domains to improve security, efficiency, and decision-making processes. By mastering K-Means for anomaly detection, you can enhance your ability to detect and address exceptional cases within your data."
  }
]